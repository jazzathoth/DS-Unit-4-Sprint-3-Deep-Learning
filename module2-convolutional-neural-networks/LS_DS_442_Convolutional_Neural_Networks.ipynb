{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "_IizNKWLomoA"
   },
   "source": [
    "<img align=\"left\" src=\"https://lever-client-logos.s3.amazonaws.com/864372b1-534c-480e-acd5-9711f850815c-1524247202159.png\" width=200>\n",
    "<br></br>\n",
    "<br></br>\n",
    "\n",
    "## *Data Science Unit 4 Sprint 4 Lesson 1*\n",
    "\n",
    "# Convolutional Neural Networks (CNNs)\n",
    "## _aka_ COMPUTER VISION!\n",
    "\n",
    "![An XKCD that hasn't quite aged so well](https://imgs.xkcd.com/comics/tasks.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('MPU2HistivI', width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0EZdBzC6pvV9"
   },
   "source": [
    "# Lecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ic_wzFnprwXI"
   },
   "source": [
    "## Transfer Learning - TensorFlow Hub\n",
    "\n",
    "\"A library for reusable machine learning modules\"\n",
    "\n",
    "This lets you quickly take advantage of a model that was trained with thousands of GPU hours. It also enables transfer learning - reusing a part of a trained model (called a module) that includes weights and assets, but also training the overall model some yourself with your own data. The advantages are fairly clear - you can use less training data, have faster training, and have a model that generalizes better.\n",
    "\n",
    "https://www.tensorflow.org/hub/\n",
    "\n",
    "**WARNING** - Dragons ahead!\n",
    "\n",
    "![Dragon](https://upload.wikimedia.org/wikipedia/commons/thumb/d/d8/Friedrich-Johann-Justin-Bertuch_Mythical-Creature-Dragon_1806.jpg/637px-Friedrich-Johann-Justin-Bertuch_Mythical-Creature-Dragon_1806.jpg)\n",
    "\n",
    "TensorFlow Hub is very bleeding edge, and while there's a good amount of documentation out there, it's not always updated or consistent. You'll have to use your problem-solving skills if you want to use it!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1878
    },
    "colab_type": "code",
    "id": "GkJUFfsgnqr_",
    "outputId": "bd761821-deb3-43fb-a397-94dacbeb1a26"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_hub'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-ac324b251159>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# TF Hub landing page example\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow_hub\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mhub\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mGraph\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_default\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_hub'"
     ]
    }
   ],
   "source": [
    "# TF Hub landing page example\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "with tf.Graph().as_default():\n",
    "  module_url = \"https://tfhub.dev/google/nnlm-en-dim128-with-normalization/1\"\n",
    "  embed = hub.Module(module_url)\n",
    "  embeddings = embed([\"A long sentence.\", \"single-word\",\n",
    "                      \"http://example.com\"])\n",
    "\n",
    "  with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    sess.run(tf.tables_initializer())\n",
    "\n",
    "    print(sess.run(embeddings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "3nJ3nIiomvFt"
   },
   "source": [
    "What are we getting? In this case, high quality sentence embeddings, with fairly little work. There also exist pretrained networks for images, which we will use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nv_QgQHFYN9t"
   },
   "source": [
    "## Image Similarity\n",
    "\n",
    "Let's use a pretrained neural network to calculate image similarity. The process is similar to the image embeddings from Basilica.ai, but we'll be running the inference ourselves (though not the training - training cutting edge models can cost tens of thousands of dollars in cloud GPU hours).\n",
    "\n",
    "First pick a module to instantiate - you can [browse modules](https://www.tensorflow.org/hub/modules/) and get fully trained state-of-the-art networks for image classification, text embeddings, and others.\n",
    "\n",
    "Let's try [DELF](https://www.tensorflow.org/hub/modules/google/delf/1), the DEep Local Features module - it is trained on photographs of landmarks, and describes input images as \"noteworthy\" points of vectors. This facilitates matching two similar images (e.g. two people taking a picture of the same landmark)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hu-25XoRmpyP"
   },
   "outputs": [],
   "source": [
    "# Step 0 - Imports\n",
    "import matplotlib.image as mpimg\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from PIL import Image, ImageOps\n",
    "from scipy.spatial import cKDTree\n",
    "from skimage.feature import plot_matches\n",
    "from skimage.measure import ransac\n",
    "from skimage.transform import AffineTransform\n",
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lb8CVOaB7kyX"
   },
   "outputs": [],
   "source": [
    "# Step 1 - Instantiate the module\n",
    "delf_module = hub.Module(\"https://tfhub.dev/google/delf/1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 419
    },
    "colab_type": "code",
    "id": "Bk1P-W757xLO",
    "outputId": "cdda7ac0-3bda-4cd7-a5f9-c52e3d798b8c"
   },
   "outputs": [],
   "source": [
    "# Step 2 - Acquire images\n",
    "import requests\n",
    "\n",
    "image_urls = [\"https://upload.wikimedia.org/wikipedia/commons/thumb/6/66/The_Leaning_Tower_of_Pisa_SB.jpeg/672px-The_Leaning_Tower_of_Pisa_SB.jpeg\",\n",
    "              \"https://www.publicdomainpictures.net/pictures/120000/velka/leaning-tower-of-pisa-1427012597XXV.jpg\"]\n",
    "\n",
    "for _id,img in enumerate(image_urls): \n",
    "    r = requests.get(img)\n",
    "    with open(f'tower{_id}.jpg', 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 249
    },
    "colab_type": "code",
    "id": "eOG7xNiB7zGN",
    "outputId": "b029bf03-ef53-48a8-f6bb-77a54d5a2768"
   },
   "outputs": [],
   "source": [
    "# Step 3 - Transform the images for DELF\n",
    "IMAGE_1_JPG = 'tower0.jpg'\n",
    "IMAGE_2_JPG = 'tower1.jpg'\n",
    "\n",
    "def resize_image(filename, new_width=256, new_height=256):\n",
    "  pil_image = Image.open(filename)\n",
    "  pil_image = ImageOps.fit(pil_image, (new_width, new_height), Image.ANTIALIAS)\n",
    "  pil_image_rgb = pil_image.convert('RGB')\n",
    "  pil_image_rgb.save(filename, format='JPEG', quality=90)\n",
    "\n",
    "resize_image(IMAGE_1_JPG)\n",
    "resize_image(IMAGE_2_JPG)\n",
    "\n",
    "def show_images(image_path_list):\n",
    "  plt.figure()\n",
    "  for i, image_path in enumerate(image_path_list):\n",
    "    plt.subplot(1, len(image_path_list), i+1)\n",
    "    plt.imshow(np.asarray(Image.open(image_path)))\n",
    "    plt.title(image_path)\n",
    "    plt.grid(False)\n",
    "    plt.yticks([])\n",
    "    plt.xticks([])\n",
    "  plt.show()\n",
    "\n",
    "show_images([IMAGE_1_JPG, IMAGE_2_JPG])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1044
    },
    "colab_type": "code",
    "id": "HwU-Byah71Ad",
    "outputId": "49d4f946-cd75-4394-cb36-1d992ca450df"
   },
   "outputs": [],
   "source": [
    "# Step 4 - Run DELF for each image\n",
    "def image_input_fn():\n",
    "  filename_queue = tf.train.string_input_producer(\n",
    "      [IMAGE_1_JPG, IMAGE_2_JPG], shuffle=False)\n",
    "  reader = tf.WholeFileReader()\n",
    "  _, value = reader.read(filename_queue)\n",
    "  image_tf = tf.image.decode_jpeg(value, channels=3)\n",
    "  return tf.image.convert_image_dtype(image_tf, tf.float32)\n",
    "\n",
    "# The module operates on a single image at a time, so define a placeholder to\n",
    "# feed an arbitrary image in.\n",
    "image_placeholder = tf.placeholder(\n",
    "    tf.float32, shape=(None, None, 3), name='input_image')\n",
    "\n",
    "module_inputs = {\n",
    "    'image': image_placeholder,\n",
    "    'score_threshold': 100.0,\n",
    "    'image_scales': [0.25, 0.3536, 0.5, 0.7071, 1.0, 1.4142, 2.0],\n",
    "    'max_feature_num': 1000,\n",
    "}\n",
    "\n",
    "module_outputs = delf_module(module_inputs, as_dict=True)\n",
    "\n",
    "image_tf = image_input_fn()\n",
    "\n",
    "with tf.train.MonitoredSession() as sess:\n",
    "  results_dict = {}  # Stores the locations and their descriptors for each image\n",
    "  for image_path in [IMAGE_1_JPG, IMAGE_2_JPG]:\n",
    "    image = sess.run(image_tf)\n",
    "    print('Extracting locations and descriptors from %s' % image_path)\n",
    "    results_dict[image_path] = sess.run(\n",
    "        [module_outputs['locations'], module_outputs['descriptors']],\n",
    "        feed_dict={image_placeholder: image})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 470
    },
    "colab_type": "code",
    "id": "Lw8XBN8r74wJ",
    "outputId": "36bf1d96-24af-415f-a9c4-779fdc06337f"
   },
   "outputs": [],
   "source": [
    "# Step 5 - Use the results to match the images\n",
    "def match_images(results_dict, image_1_path, image_2_path):\n",
    "  distance_threshold = 0.8\n",
    "\n",
    "  # Read features.\n",
    "  locations_1, descriptors_1 = results_dict[image_1_path]\n",
    "  num_features_1 = locations_1.shape[0]\n",
    "  print(\"Loaded image 1's %d features\" % num_features_1)\n",
    "  locations_2, descriptors_2 = results_dict[image_2_path]\n",
    "  num_features_2 = locations_2.shape[0]\n",
    "  print(\"Loaded image 2's %d features\" % num_features_2)\n",
    "\n",
    "  # Find nearest-neighbor matches using a KD tree.\n",
    "  d1_tree = cKDTree(descriptors_1)\n",
    "  _, indices = d1_tree.query(\n",
    "      descriptors_2, distance_upper_bound=distance_threshold)\n",
    "\n",
    "  # Select feature locations for putative matches.\n",
    "  locations_2_to_use = np.array([\n",
    "      locations_2[i,]\n",
    "      for i in range(num_features_2)\n",
    "      if indices[i] != num_features_1\n",
    "  ])\n",
    "  locations_1_to_use = np.array([\n",
    "      locations_1[indices[i],]\n",
    "      for i in range(num_features_2)\n",
    "      if indices[i] != num_features_1\n",
    "  ])\n",
    "\n",
    "  # Perform geometric verification using RANSAC.\n",
    "  _, inliers = ransac(\n",
    "      (locations_1_to_use, locations_2_to_use),\n",
    "      AffineTransform,\n",
    "      min_samples=3,\n",
    "      residual_threshold=20,\n",
    "      max_trials=1000)\n",
    "\n",
    "  print('Found %d inliers' % sum(inliers))\n",
    "\n",
    "  # Visualize correspondences.\n",
    "  _, ax = plt.subplots(figsize=(9, 18))\n",
    "  img_1 = mpimg.imread(image_1_path)\n",
    "  img_2 = mpimg.imread(image_2_path)\n",
    "  inlier_idxs = np.nonzero(inliers)[0]\n",
    "  plot_matches(\n",
    "      ax,\n",
    "      img_1,\n",
    "      img_2,\n",
    "      locations_1_to_use,\n",
    "      locations_2_to_use,\n",
    "      np.column_stack((inlier_idxs, inlier_idxs)),\n",
    "      matches_color='b')\n",
    "  ax.axis('off')\n",
    "  ax.set_title('DELF correspondences')\n",
    "\n",
    "match_images(results_dict, IMAGE_1_JPG, IMAGE_2_JPG)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "PdlSieKJ7_XR"
   },
   "source": [
    "This lets us visualize a little bit better what it means to e.g. calculate the \"embeddings\" of an image, and then compare the values to embeddings of another image in order to calculate similarity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "wVcp_myWW01-"
   },
   "source": [
    "## Convolutional Neural Networks\n",
    "\n",
    "Like neural networks themselves, CNNs are inspired by biology - specifically, the receptive fields of the visual cortex."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 422
    },
    "colab_type": "code",
    "id": "tm33Gfl5W8QL",
    "outputId": "0b5af969-4671-4c27-cb7b-6a454d7021af"
   },
   "outputs": [],
   "source": [
    "from IPython.display import YouTubeVideo\n",
    "YouTubeVideo('IOHayh06LJ4', width=600, height=400)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "tXWJXgPJXrb4"
   },
   "source": [
    "Put roughly, in a real brain the neurons in the visual cortex *specialize* to be receptive to certain regions, shapes, colors, orientations, and other common visual features. In a sense, the very structure of our cognitive system transforms raw visual input, and sends it to neurons that specialize in handling particular subsets of it.\n",
    "\n",
    "CNNs imitate this approach by applying a convolution. A convolution is an operation on two functions that produces a third function, showing how one function modifies another. Convolutions have a [variety of nice mathematical properties](https://en.wikipedia.org/wiki/Convolution#Properties) - commutativity, associativity, distributivity, and more. Applying a convolution effectively transforms the \"shape\" of the input.\n",
    "\n",
    "One common confusion - the term \"convolution\" is used to refer to both the process of computing the third (joint) function and the process of applying it. In our context, it's more useful to think of it as an application, again loosely analogous to the mapping from visual field to receptive areas of the cortex in a real animal.\n",
    "\n",
    "### Convolution - an example\n",
    "\n",
    "Consider blurring an image - assume the image is represented as a matrix of numbers, where each number corresponds to the color value of a pixel.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "OsAcbKvoeaqU",
    "outputId": "dbb28705-36c7-4691-f7df-e9f82e3ee91e"
   },
   "outputs": [],
   "source": [
    "import imageio\n",
    "import matplotlib.pyplot as plt\n",
    "from skimage import color, io\n",
    "from skimage.exposure import rescale_intensity\n",
    "\n",
    "austen = io.imread('https://dl.airtable.com/S1InFmIhQBypHBL0BICi_austen.jpg')\n",
    "austen_grayscale = rescale_intensity(color.rgb2gray(austen))\n",
    "austen_grayscale"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "HC-JAtkOe13a",
    "outputId": "bdeda9e0-32a1-419d-ed89-fe4222a3eece"
   },
   "outputs": [],
   "source": [
    "austen_grayscale.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "JL0GgUTcfT0S",
    "outputId": "7faeb809-9690-4c07-ed01-ec98afd5032d"
   },
   "outputs": [],
   "source": [
    "plt.imshow(austen_grayscale, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zNhHIeq5e4K8"
   },
   "source": [
    "One way to blur would be to replace each point with the average of itself and its neighbors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "4vWpKpRFgTR8",
    "outputId": "557fcff7-b5ff-448a-c5df-356c71312ce7"
   },
   "outputs": [],
   "source": [
    "austen_blur1 = austen_grayscale.copy()\n",
    "\n",
    "for i, row in enumerate(austen_grayscale):\n",
    "  for j, col in enumerate(row):\n",
    "    # Bit of logic to handle edges - only doing direct non-diag neighbors\n",
    "    blur_val = col\n",
    "    num_averaged = 1\n",
    "    if i > 0:\n",
    "      blur_val += austen_grayscale[i - 1][j]\n",
    "      num_averaged += 1\n",
    "    if j > 0:\n",
    "      blur_val += austen_grayscale[i][j - 1]\n",
    "      num_averaged += 1\n",
    "    if i < len(austen_grayscale) - 1:\n",
    "      blur_val += austen_grayscale[i + 1][j]\n",
    "      num_averaged += 1\n",
    "    if j < len(row) - 1:\n",
    "      blur_val += austen_grayscale[i][j + 1]\n",
    "      num_averaged += 1\n",
    "    austen_blur1[i][j] = blur_val / num_averaged\n",
    "\n",
    "austen_blur1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 351
    },
    "colab_type": "code",
    "id": "KN-ibr_DhyaV",
    "outputId": "241716ac-3415-4cfd-9602-0dd59a80ed47"
   },
   "outputs": [],
   "source": [
    "plt.imshow(austen_blur1, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jm_3hOZBjAat"
   },
   "source": [
    "What if we wanted to blur by averaging a larger neighborhood of pixels, but treating the closer neighbors as more important than the far ones?\n",
    "\n",
    "One natural approach would be to use a two dimensional Normal distribution to determine the appropriate pixel weights for averaging."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "U49w-RZdjACW",
    "outputId": "983760d7-41b3-4e3d-849f-7c3aa90b8e4a"
   },
   "outputs": [],
   "source": [
    "from skimage.filters import gaussian\n",
    "\n",
    "# Using relatively large sigma so the filter impact is clear\n",
    "austen_blur2 = gaussian(austen_grayscale, sigma=5)\n",
    "austen_blur2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "heFshJrskYcu",
    "outputId": "8dba3372-4cb2-426d-f30a-2dbc4c488fee"
   },
   "outputs": [],
   "source": [
    "plt.imshow(austen_blur2, cmap=\"gray\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-7Pquhe6j2On"
   },
   "source": [
    "This is called a *Gaussian blur*, and is an early and well-established application of convolutions.\n",
    "\n",
    "![Gaussian blur applied to newspaper image](https://upload.wikimedia.org/wikipedia/commons/d/d7/Halftone%2C_Gaussian_Blur.jpg)\n",
    "\n",
    "Also - it is a **convolution**!\n",
    "\n",
    "Specifically, it's the application of the two dimensional Gaussian function to the function that is the image itself (something that takes values $x, y$ and returns a number for the value of that pixel).\n",
    "\n",
    "And that's really it - there are of course many more convolutions out there. Following is a relatively simple form of edge detection as a convolution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 243
    },
    "colab_type": "code",
    "id": "QopB0uo6lNxq",
    "outputId": "2364bf3d-8fb9-487a-d2db-eb794939c77a"
   },
   "outputs": [],
   "source": [
    "import scipy.ndimage as nd\n",
    "\n",
    "edge_convolution = np.array([[1,1,1,1,1],\n",
    "                             [0,0,0,0,0],\n",
    "                             [0,0,0,0,0],\n",
    "                             [0,0,0,0,0],\n",
    "                             [-1,-1,-1,-1,-1]])\n",
    "\n",
    "austen_edges = nd.convolve(austen_grayscale, edge_convolution)\n",
    "austen_edges"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 368
    },
    "colab_type": "code",
    "id": "-LwEpFW1l-6b",
    "outputId": "51b9bdf4-dab6-406a-f98b-fd0a7b480859"
   },
   "outputs": [],
   "source": [
    "plt.imshow(austen_edges, cmap=\"gray\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OOep4ugw8coa"
   },
   "source": [
    "## A Typical CNN\n",
    "\n",
    "![A Typical CNN](https://upload.wikimedia.org/wikipedia/commons/thumb/6/63/Typical_cnn.png/800px-Typical_cnn.png)\n",
    "\n",
    "The first stage of a CNN is, unsurprisingly, a convolution - specifically, a transformation that maps regions of the input image to neurons responsible for receiving them. The convolutional layer can be visualized as follows:\n",
    "\n",
    "![Convolutional layer](https://upload.wikimedia.org/wikipedia/commons/6/68/Conv_layer.png)\n",
    "\n",
    "The red represents the original input image, and the blue the neurons that correspond.\n",
    "\n",
    "As shown in the first image, a CNN can have multiple rounds of convolutions, [downsampling](https://en.wikipedia.org/wiki/Downsampling_(signal_processing)) (a digital signal processing technique that effectively reduces the information by passing through a filter), and then eventually a fully connected neural network and output layer. Typical output layers for a CNN would be oriented towards classification or detection problems - e.g. \"does this picture contain a cat, a dog, or some other animal?\"\n",
    "\n",
    "Why are CNNs so popular?\n",
    "\n",
    "1. They work, really well (see XKCD at top)\n",
    "2. Compared to prior image learning techniques, they require relatively little image preprocessing (cropping/centering, normalizing, etc.)\n",
    "3. Relatedly, they are *robust* to all sorts of common problems in images (shifts, lighting, etc.)\n",
    "\n",
    "Actually training a cutting edge image classification CNN is nontrivial computationally - the good news is, with transfer learning, we can get one \"off-the-shelf\"!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "FM_ApKbGYM9S",
    "outputId": "4bfd7ce4-47e5-4320-d1b8-2b20e9f66416"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "from keras.applications.resnet50 import ResNet50\n",
    "from keras.preprocessing import image\n",
    "from keras.applications.resnet50 import preprocess_input, decode_predictions\n",
    "\n",
    "def process_img_path(img_path):\n",
    "  return image.load_img(img_path, target_size=(224, 224))\n",
    "\n",
    "def img_contains_banana(img):\n",
    "  x = image.img_to_array(img)\n",
    "  x = np.expand_dims(x, axis=0)\n",
    "  x = preprocess_input(x)\n",
    "  model = ResNet50(weights='imagenet')\n",
    "  features = model.predict(x)\n",
    "  results = decode_predictions(features, top=3)[0]\n",
    "  print(results)\n",
    "  for entry in results:\n",
    "    if entry[1] == 'banana':\n",
    "      return entry[2]\n",
    "  return 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 593
    },
    "colab_type": "code",
    "id": "_cQ8ZsJF_Z3B",
    "outputId": "02545656-8773-4bb2-9ff5-36d8c658dc00"
   },
   "outputs": [],
   "source": [
    "image_urls = [\"https://github.com/LambdaSchool/ML-YouOnlyLookOnce/raw/master/sample_data/negative_examples/example11.jpeg\",\n",
    "              \"https://github.com/LambdaSchool/ML-YouOnlyLookOnce/raw/master/sample_data/positive_examples/example0.jpeg\"]\n",
    "\n",
    "for _id,img in enumerate(image_urls): \n",
    "    r = requests.get(img)\n",
    "    with open(f'example{_id}.jpg', 'wb') as f:\n",
    "        f.write(r.content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "id": "Gxzkai0q_d-4",
    "outputId": "a6bd9b95-9665-4df0-c74d-3d4e876eaf48"
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename='example0.jpeg', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "X8NIlClb_n8s",
    "outputId": "7c9b9f98-073e-4ab0-a336-e3fc89fa8439"
   },
   "outputs": [],
   "source": [
    "img_contains_banana(process_img_path('example11.jpeg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 417
    },
    "colab_type": "code",
    "id": "YIwtRazQ_tQr",
    "outputId": "7be6599b-253d-4600-e1f5-ac0ab0f2dfbc"
   },
   "outputs": [],
   "source": [
    "Image(filename='example0.jpeg', width=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "GDXwkPWOAB14",
    "outputId": "6493a0cb-b57b-43be-8a4e-ac06e51bdada"
   },
   "outputs": [],
   "source": [
    "img_contains_banana(process_img_path('example0.jpeg'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "CdF5A88oPYvX"
   },
   "source": [
    "Notice that, while it gets it right, the confidence for the banana image is fairly low. That's because so much of the image is \"not-banana\"! How can this be improved? Bounding boxes to center on items of interest (see extended resources)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "0lfZdD_cp1t5"
   },
   "source": [
    "# Assignment\n",
    "\n",
    "Load a pretrained network from TensorFlow Hub, [ResNet50](https://tfhub.dev/google/imagenet/resnet_v1_50/classification/1) - a 50 layer deep network trained to recognize [1000 objects](https://storage.googleapis.com/download.tensorflow.org/data/ImageNetLabels.txt). Starting usage:\n",
    "\n",
    "```python\n",
    "module = hub.Module(\"https://tfhub.dev/google/imagenet/resnet_v1_50/classification/1\")\n",
    "height, width = hub.get_expected_image_size(module)\n",
    "images = ...  # A batch of images with shape [batch_size, height, width, 3].\n",
    "logits = module(images)  # Logits with shape [batch_size, num_classes].\n",
    "```\n",
    "\n",
    "Apply it to classify the images downloaded below (images from a search for animals in national parks):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 245
    },
    "colab_type": "code",
    "id": "GgTukFacGBBs",
    "outputId": "d8ebf892-a091-4ab5-98a3-9bb7ceddfd53"
   },
   "outputs": [],
   "source": [
    "!pip install google_images_download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 853
    },
    "colab_type": "code",
    "id": "h6sMrlvLKT5X",
    "outputId": "0beade70-7047-45cc-d40c-6105ef967dd7"
   },
   "outputs": [],
   "source": [
    "from google_images_download import google_images_download\n",
    "\n",
    "response = google_images_download.googleimagesdownload()\n",
    "arguments = {\"keywords\": \"animal national park\", \"limit\": 20,\n",
    "             \"print_urls\": True}\n",
    "absolute_image_paths = response.download(arguments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 364
    },
    "colab_type": "code",
    "id": "zKaJ3uOiMAv0",
    "outputId": "2d27e452-1ec2-4c88-a038-8bfc4d041f70"
   },
   "outputs": [],
   "source": [
    "absolute_image_paths"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Hly75VuiMQE1"
   },
   "source": [
    "Report both the most likely estimated class for any image, and also investigate (a) images where the classifier isn't that certain (the best estimate is low), and (b) images where the classifier fails.\n",
    "\n",
    "Answer (in writing in the notebook) the following - \"What sorts of images do CNN classifiers do well with? What sorts do they not do so well? And what are your hypotheses for why?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zE4a4O7Bp5x1"
   },
   "source": [
    "# Resources and Stretch Goals"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "uT3UV3gap9H6"
   },
   "source": [
    "Stretch goals\n",
    "- Enhance your code to use classes/functions and accept terms to search and classes to look for in recognizing the downloaded images (e.g. download images of parties, recognize all that contain balloons)\n",
    "- Check out [other available pretrained networks](https://tfhub.dev), try some and compare\n",
    "- Image recognition/classification is somewhat solved, but *relationships* between entities and describing an image is not - check out some of the extended resources (e.g. [Visual Genome](https://visualgenome.org/)) on the topic\n",
    "- Transfer learning - using images you source yourself, [retrain a classifier](https://www.tensorflow.org/hub/tutorials/image_retraining) with a new category\n",
    "- (Not CNN related) Use [piexif](https://pypi.org/project/piexif/) to check out the metadata of images passed in to your system - see if they're from a national park! (Note - many images lack GPS metadata, so this won't work in most cases, but still cool)\n",
    "\n",
    "Resources\n",
    "- [Deep Residual Learning for Image Recognition](https://arxiv.org/abs/1512.03385) - influential paper (introduced ResNet)\n",
    "- [YOLO: Real-Time Object Detection](https://pjreddie.com/darknet/yolo/) - an influential convolution based object detection system, focused on inference speed (for applications to e.g. self driving vehicles)\n",
    "- [R-CNN, Fast R-CNN, Faster R-CNN, YOLO](https://towardsdatascience.com/r-cnn-fast-r-cnn-faster-r-cnn-yolo-object-detection-algorithms-36d53571365e) - comparison of object detection systems\n",
    "- [Common Objects in Context](http://cocodataset.org/) - a large-scale object detection, segmentation, and captioning dataset\n",
    "- [Visual Genome](https://visualgenome.org/) - a dataset, a knowledge base, an ongoing effort to connect structured image concepts to language"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "LS_DS_442_Convolutional_Neural_Networks.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
